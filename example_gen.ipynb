{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a9003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/envs/plp_env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d0b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Salesforce/codegen-350M-multi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f6cd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee775a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "_=model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dab2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\"from\",\n",
    "               \"from transformers import AutoTokenizer, AutoModelForCausalLM\", \n",
    "               \"tokenizer = AutoTokenizer.from_pretrained(model_name)\", \n",
    "               \"model = AutoModelForCausalLM.from_pretrained(model_name)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d8a04fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cuda(inputs):\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].cuda()\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11aefa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = { 'min_length':32, \n",
    "#            'max_length':128, \n",
    "#            'do_sample':True, \n",
    "#            'top_p':0.95, \n",
    "#            'num_return_sequences':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c47d6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { \"max_length\": 128, \"do_sample\":False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edfd890",
   "metadata": {},
   "source": [
    "# batch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a4c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_with_pad = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_with_pad.pad_token_id = tokenizer_with_pad.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efbb6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts = tokenizer_with_pad(input_texts, return_tensors=\"pt\", padding=True)\n",
    "encoded_texts = set_cuda(inputs = encoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ff4015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/envs/plp_env/lib/python3.7/site-packages/transformers/models/codegen/modeling_codegen.py:167: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484801627/work/aten/src/ATen/native/TensorCompare.cpp:402.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.83 s, sys: 106 ms, total: 1.94 s\n",
      "Wall time: 1.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**encoded_texts, \n",
    "                                   pad_token_id=tokenizer_with_pad.eos_token_id,\n",
    "                                   **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaff8c8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT 1:\n",
      "\n",
      "from<<<|cpp|>/*\n",
      " * Copyright (C) 2008-2013 TrinityCore <http://www.trinitycore.org/>\n",
      " * Copyright (C) 2006-2009 ScriptDev2 <https://scriptdev2.svn.sourceforge.net/>\n",
      " *\n",
      " * This program is free software; you can redistribute it and/or modify it\n",
      " * under the terms of the GNU General Public License as published by the\n",
      " * Free Software Foundation; either version 2 of the License, or (at your\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the generation for thr 1st sample\n",
    "for i, generated_text in enumerate(tokenizer_with_pad.batch_decode(generated_ids, skip_special_tokens=True)):\n",
    "    print(f\"TEXT {i+1}:\")\n",
    "    print()\n",
    "    print(generated_text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44039a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check attention_mask for the 1st sample (eos_token_id == pad_token_id == 50256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "337fd0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6738, 1) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0) (50256, 0)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(*zip(encoded_texts['input_ids'][i].tolist(), encoded_texts['attention_mask'][i].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "844aa19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check attention_mask for the 2st sample (eos_token_id == pad_token_id == 50256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78c8d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6738, 1) (6121, 1) (364, 1) (1330, 1) (11160, 1) (30642, 1) (7509, 1) (11, 1) (11160, 1) (17633, 1) (1890, 1) (24334, 1) (6775, 1) (31288, 1) (50256, 0) (50256, 0) (50256, 0) (50256, 0)\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "print(*zip(encoded_texts['input_ids'][i].tolist(), encoded_texts['attention_mask'][i].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bc2c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment - add more padded tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d2c0270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18]) torch.Size([18])\n",
      "torch.Size([1, 38]) torch.Size([1, 38])\n"
     ]
    }
   ],
   "source": [
    "extra_len = 20\n",
    "\n",
    "input_ids0 = encoded_texts['input_ids'][0]\n",
    "attention_mask0 = encoded_texts['attention_mask'][0]\n",
    "\n",
    "print(input_ids0.shape, attention_mask0.shape)\n",
    "\n",
    "extra_input_ids = tokenizer_with_pad.pad_token_id*torch.ones(size=(extra_len,), dtype=torch.int)\n",
    "new_input_ids0 = torch.hstack([input_ids0, extra_input_ids.cuda()]).view((1,-1))\n",
    "\n",
    "extra_attention_mask = torch.zeros(size=(extra_len,), dtype=torch.int)\n",
    "new_attention_mask0 = torch.hstack([attention_mask0, extra_attention_mask.cuda()]).view((1,-1))\n",
    "\n",
    "print(new_input_ids0.shape, new_attention_mask0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f66981de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 0 ns, total: 1.18 s\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    generated_ids0 = model.generate(input_ids=new_input_ids0, attention_mask=new_attention_mask0,\n",
    "                                   pad_token_id=tokenizer_with_pad.eos_token_id,\n",
    "                                   **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "488c13aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6738, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,    27,    27,\n",
       "            91, 20322,    91,    29, 15211,   198,  1635, 15069,   357,    34,\n",
       "             8,  3648,    12,  6390, 22844, 14055,  1279,  4023,  1378,  2503,\n",
       "            13,  2213,  6269,  7295,    13,  2398, 15913,   198,  1635, 15069,\n",
       "           357,    34,     8,  4793,    12, 10531, 12327, 13603,    17,  1279,\n",
       "          5450,  1378, 12048,  7959,    17,    13, 21370,    77,    13, 10459,\n",
       "         30293,    13,  3262, 15913,   198,  1635,   198,  1635,   770,  1430,\n",
       "           318,  1479,  3788,    26,   345,   460, 17678,  4163,   340,   290,\n",
       "            14,   273, 13096,   340,   198,  1635,   739,   262,  2846,   286,\n",
       "           262, 22961,  3611,  5094, 13789,   355,  3199,   416]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c4deffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6738, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,    27,    27,\n",
       "           27,    91, 20322,    91,    29, 15211,   198,  1635, 15069,   357,\n",
       "           34,     8,  3648,    12,  6390, 22844, 14055,  1279,  4023,  1378,\n",
       "         2503,    13,  2213,  6269,  7295,    13,  2398, 15913,   198,  1635,\n",
       "        15069,   357,    34,     8,  4793,    12, 10531, 12327, 13603,    17,\n",
       "         1279,  5450,  1378, 12048,  7959,    17,    13, 21370,    77,    13,\n",
       "        10459, 30293,    13,  3262, 15913,   198,  1635,   198,  1635,   770,\n",
       "         1430,   318,  1479,  3788,    26,   345,   460, 17678,  4163,   340,\n",
       "          290,    14,   273, 13096,   340,   198,  1635,   739,   262,  2846,\n",
       "          286,   262, 22961,  3611,  5094, 13789,   355,  3199,   416,   262,\n",
       "          198,  1635,  3232, 10442,  5693,    26,  2035,  2196,   362,   286,\n",
       "          262, 13789,    11,   393,   357,   265,   534,   198],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f750a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add more padded tokens and get different result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb5b8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce120a63",
   "metadata": {},
   "source": [
    "# single mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20afdb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f96ab75d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========1 text ==============\n",
      "from __future__ import unicode_literals\n",
      "\n",
      "from django.db import models\n",
      "from django.utils.encoding import python_2_unicode_compatible\n",
      "from django.utils.translation import ugettext_lazy as _\n",
      "\n",
      "from. import models\n",
      "\n",
      "\n",
      "@python_2_unicode_compatible\n",
      "class User(models.Model):\n",
      "    username = models.CharField(_('username'), max_length=30)\n",
      "    password = models.CharField(_('password'), max_length=30)\n",
      "    email = models.EmailField(_('email address'),\n",
      "\n",
      "CPU times: user 1.65 s, sys: 0 ns, total: 1.65 s\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, input_text in enumerate(input_texts):\n",
    "    encoded_text = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    encoded_text = set_cuda(inputs = encoded_text)\n",
    "    with torch.no_grad():\n",
    "        generated_single_ids = model.generate(**encoded_text,\n",
    "                                              pad_token_id=tokenizer.eos_token_id,\n",
    "                                               **params)\n",
    "    for decoded_single_text in tokenizer.batch_decode(generated_single_ids, skip_special_tokens=True):\n",
    "        print(f\"=========={i+1} text ==============\")\n",
    "        print(decoded_single_text)\n",
    "        print()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1536015f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6738, 11593, 37443,   834,  1330, 28000,  1098,    62, 17201,   874,\n",
       "           198,   198,  6738, 42625, 14208,    13,  9945,  1330,  4981,   198,\n",
       "          6738, 42625, 14208,    13, 26791,    13, 12685,  7656,  1330, 21015,\n",
       "            62,    17,    62, 46903,  1098,    62, 38532,   198,  6738, 42625,\n",
       "         14208,    13, 26791,    13, 41519,  1330,   334,  1136,  5239,    62,\n",
       "            75, 12582,   355,  4808,   198,   198,  6738,   764,  1330,  4981,\n",
       "           628,   198,    31, 29412,    62,    17,    62, 46903,  1098,    62,\n",
       "         38532,   198,  4871, 11787,     7, 27530,    13, 17633,  2599,   198,\n",
       "         50284, 29460,   796,  4981,    13, 12441, 15878, 28264, 10786, 29460,\n",
       "         33809,  3509,    62, 13664,    28,  1270,     8,   198, 50284, 28712,\n",
       "           796,  4981,    13, 12441, 15878, 28264, 10786, 28712, 33809,  3509,\n",
       "            62, 13664,    28,  1270,     8,   198, 50284, 12888,   796,  4981,\n",
       "            13, 15333, 15878, 28264, 10786, 12888,  2209, 33809]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_single_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a63b0",
   "metadata": {},
   "source": [
    "# question (why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f8794a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for single mode we have got:\n",
    "\n",
    "# from __future__ import unicode_literals\n",
    "\n",
    "# from django.db import models\n",
    "# from django.utils.encoding import python_2_unicode_compatible\n",
    "# from django.utils.translation import ugettext_lazy as _\n",
    "\n",
    "# from. import models\n",
    "\n",
    "\n",
    "# @python_2_unicode_compatible\n",
    "# class User(models.Model):\n",
    "#     username = models.CharField(_('username'), max_length=30)\n",
    "#     password = models.CharField(_('password'), max_length=30)\n",
    "#     email = models.EmailField(_('email address'),\n",
    "                              \n",
    " #for batch mopde we have got:   \n",
    "                              \n",
    "# from<<<|cpp|>/*\n",
    "#  * Copyright (C) 2008-2013 TrinityCore <http://www.trinitycore.org/>\n",
    "#  * Copyright (C) 2006-2009 ScriptDev2 <https://scriptdev2.svn.sourceforge.net/>\n",
    "#  *\n",
    "#  * This program is free software; you can redistribute it and/or modify it\n",
    "#  * under the terms of the GNU General Public License as published by the\n",
    "#  * Free Software Foundation; either version 2 of the License, or (at your"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb02873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
